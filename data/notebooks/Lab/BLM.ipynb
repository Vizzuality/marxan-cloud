{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLM calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps in this pipeline:\n",
    "1. Run the same scenario 6 times, 10 runs each, modifying the BLM value according to: \n",
    "BLM = Z*sqrt(PU area)\n",
    "(Z = 0.001, 0.01, 0.1,1, 10 and 100)  \n",
    "(Helper functions to run Marxan are at the bottom)\n",
    "\n",
    "2. Spatial plot the best solution of each of these scenario outputs (Visually inspect changes in BLM)\n",
    "3. Plot Boundary Length vs Cost (Curve)\n",
    "4. Calculate the optimum BLM value that minimizes both cost and Boundary length (clumping)\n",
    "\n",
    "*The planning units grid shapefile is requiered for the Boundary length calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estimate BLM testing values (6 scenarios keeping all inputs equal except BLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARXAN_FOLDER = '/home/jovyan/work/datasets/raw/Marxan_Targets'\n",
    "MARXAN_EXECUTABLE = f'{MARXAN_FOLDER}/MarOpt_v243_Linux64'\n",
    "MARXAN_INPUTDATA = 'input.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PU Area\n",
    "pu_area = gpd.read_file(f'{MARXAN_FOLDER}/pu/pulayer.shp')\n",
    "#pu_area.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLM values to start calibration\n",
    "# Rule of thumb: Z = pu_area*[0.01,0.1,1,10,100]\n",
    "blm_range= [0.001,0.01,0.1,1,10,100]\n",
    "area = pu_area['geometry'].map(lambda p: p.area / 10**6).mean()\n",
    "blm_values = [element * math.sqrt(area) for element in blm_range]\n",
    "blm_dict = dict(zip(blm_range, blm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Marxan 6 times with those BLM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_marxanProject(path):\n",
    "    os.mkdir(f'{path}/input')\n",
    "    os.mkdir(f'{path}/output')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLM_0.002\n",
      "Marxan v 2.43\n",
      "BLM_0.02\n",
      "Marxan v 2.43\n",
      "BLM_0.2\n",
      "Marxan v 2.43\n",
      "\n",
      "BLM_2.0\n",
      "Marxan v 2.43\n",
      "BLM_20.0\n",
      "Marxan v 2.43\n",
      "BLM_200.0\n",
      "Marxan v 2.43\n"
     ]
    }
   ],
   "source": [
    "### RUN MARXAN 6 times\n",
    "blm_folder=[]\n",
    "InputFile = DatFile(f'{MARXAN_FOLDER}/{MARXAN_INPUTDATA}')\n",
    "InputFile.read()\n",
    "\n",
    "for blm in blm_values:\n",
    "    print(f'BLM_{blm}')\n",
    "    userInputFile = inputDatFile.from_dat(InputFile.data)\n",
    "    \n",
    "    ## Modify for BLM calculations and save as new input.dat\n",
    "    userInputFile.BLM = blm\n",
    "    userInputFile.OUTPUTDIR =f'BLM_{blm}'\n",
    "    userInputFile.NUMREPS =10\n",
    "    \n",
    "    userInputFile_df = pd.DataFrame.from_dict(userInputFile.__dict__, orient='index')\n",
    "    userInputFile_df.drop('BLOCKDEFNAME', inplace=True)\n",
    "\n",
    "    CreateFileFromDF(f'{MARXAN_FOLDER}/{MARXAN_INPUTDATA}',userInputFile_df, inputDatFile)\n",
    "    if not os.path.exists(f'{MARXAN_FOLDER}/{userInputFile.OUTPUTDIR}'):\n",
    "        os.mkdir(f'{MARXAN_FOLDER}/{userInputFile.OUTPUTDIR}')\n",
    "    \n",
    "    blm_folder.append(f'{userInputFile.OUTPUTDIR}')\n",
    "    \n",
    "    #EXECUTES MARXAN\n",
    "    # Needs to execute marxan from the marxan root folder in order to make the file find the required data.\n",
    "    os.chdir(MARXAN_FOLDER)\n",
    "    with subprocess.Popen([MARXAN_EXECUTABLE],\n",
    "                             stdout=subprocess.PIPE,\n",
    "                             stderr=subprocess.STDOUT,\n",
    "                             universal_newlines=True,\n",
    "                          bufsize=-1) as process:\n",
    "        while process.poll() is None:\n",
    "            output = process.stdout.readline()\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "\n",
    "    os.chdir('/home/jovyan/work/notebooks/Lab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a plot from the best solution of each BLM scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLM_0.002', 'BLM_0.02', 'BLM_0.2', 'BLM_2.0', 'BLM_20.0', 'BLM_200.0']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/work/datasets/raw/Marxan_Targets/BLM_0.002/output_best.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-70a07746e061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblm_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m321\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{MARXAN_FOLDER}/{blm}/output_best.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msolution_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpu_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PUID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PUID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msolution_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SOLUTION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/work/datasets/raw/Marxan_Targets/BLM_0.002/output_best.csv'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAADDCAYAAADaxDfvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAL50lEQVR4nO3dX4hc93nG8e9TKYLESWMTbUKqP1QtSmy12MWeuCakrVPTRlIvRMAXlkNMTUAI7JBLm14kBd80F4UQ/EcsRpjcRDcxqVIUm9KSuOAo0Qps2bKx2crE2ihgOQ4pOFCz9tuLmcbj6a737GhWuz/N9wMLc855Z+Z9mZ1nzzl7ZjdVhSRtdL+33g1IUheGlaQmGFaSmmBYSWqCYSWpCYaVpCasGFZJjiZ5Lcnzy2xPkm8nmU9yJsmNk29T0rTrsmf1GLD3fbbvA3YPvg4Bj1x6W5L0XiuGVVU9BbzxPiUHgO9U30ng6iSfnFSDkgSTOWe1DTg/tLwwWCdJE7N5Ao+RJdYt+RmeJIfoHypy1VVX3XTttddO4OklteL06dOvV9XMOPedRFgtADuGlrcDF5YqrKpZYBag1+vV3NzcBJ5eUiuS/Hzc+07iMPA4cNfgt4K3AL+pql9O4HEl6XdW3LNK8l3gVmBrkgXgG8AHAKrqCHAC2A/MA78F7l6rZiVNrxXDqqoOrrC9gHsm1pEkLcEr2CU1wbCS1ATDSlITDCtJTTCsJDXBsJLUBMNKUhMMK0lNMKwkNcGwktQEw0pSEwwrSU0wrCQ1wbCS1ATDSlITDCtJTTCsJDXBsJLUBMNKUhMMK0lNMKwkNcGwktQEw0pSEzqFVZK9SV5KMp/k/iW2fzTJD5I8m+RsEv/RqaSJWjGskmwCHgL2AXuAg0n2jJTdA7xQVTfQ/+/N/5xky4R7lTTFuuxZ3QzMV9W5qnoLOAYcGKkp4CNJAnwYeANYnGinkqZal7DaBpwfWl4YrBv2IHAdcAF4DvhaVb0zkQ4liW5hlSXW1cjyF4BngD8A/gx4MMnv/78HSg4lmUsyd/HixVW2KmmadQmrBWDH0PJ2+ntQw+4GHq++eeAV4NrRB6qq2arqVVVvZmZm3J4lTaEuYXUK2J1k1+Ck+R3A8ZGaV4HbAJJ8Avg0cG6SjUqabptXKqiqxST3Ak8Cm4CjVXU2yeHB9iPAA8BjSZ6jf9h4X1W9voZ9S5oyK4YVQFWdAE6MrDsydPsC8LeTbU2S3uUV7JKaYFhJaoJhJakJhpWkJhhWkppgWElqgmElqQmGlaQmGFaSmmBYSWqCYSWpCYaVpCYYVpKaYFhJaoJhJakJhpWkJhhWkppgWElqgmElqQmGlaQmGFaSmmBYSWqCYSWpCZ3CKsneJC8lmU9y/zI1tyZ5JsnZJD+ebJuSpt2K/+Q0ySbgIeBvgAXgVJLjVfXCUM3VwMPA3qp6NcnH16hfSVOqy57VzcB8VZ2rqreAY8CBkZo7gcer6lWAqnptsm1KmnZdwmobcH5oeWGwbtingGuS/CjJ6SR3TapBSYIOh4FAllhXSzzOTcBtwAeBnyQ5WVUvv+eBkkPAIYCdO3euvltJU6vLntUCsGNoeTtwYYmaJ6rqzap6HXgKuGH0gapqtqp6VdWbmZkZt2dJU6hLWJ0CdifZlWQLcAdwfKTmX4C/SLI5yYeAPwdenGyrkqbZioeBVbWY5F7gSWATcLSqziY5PNh+pKpeTPIEcAZ4B3i0qp5fy8YlTZdUjZ5+ujx6vV7Nzc2ty3NLWh9JTldVb5z7egW7pCYYVpKaYFhJaoJhJakJhpWkJhhWkppgWElqgmElqQmGlaQmGFaSmmBYSWqCYSWpCYaVpCYYVpKaYFhJaoJhJakJhpWkJhhWkppgWElqgmElqQmGlaQmGFaSmmBYSWpCp7BKsjfJS0nmk9z/PnWfSfJ2ktsn16IkdQirJJuAh4B9wB7gYJI9y9R9k/5/bpakieqyZ3UzMF9V56rqLeAYcGCJuq8C3wNem2B/kgR0C6ttwPmh5YXBut9Jsg34InBkcq1J0ru6hFWWWFcjy98C7quqt9/3gZJDSeaSzF28eLFji5IEmzvULAA7hpa3AxdGanrAsSQAW4H9SRar6vvDRVU1C8wC9Hq90cCTpGV1CatTwO4ku4BfAHcAdw4XVNWu/7ud5DHgX0eDSpIuxYphVVWLSe6l/1u+TcDRqjqb5PBgu+epJK25LntWVNUJ4MTIuiVDqqr+/tLbkqT38gp2SU0wrCQ1wbCS1ATDSlITDCtJTTCsJDXBsJLUBMNKUhMMK0lNMKwkNcGwktQEw0pSEwwrSU0wrCQ1wbCS1ATDSlITDCtJTTCsJDXBsJLUBMNKUhMMK0lNMKwkNcGwktSETmGVZG+Sl5LMJ7l/ie1fSnJm8PV0khsm36qkabZiWCXZBDwE7AP2AAeT7BkpewX4q6q6HngAmJ10o5KmW5c9q5uB+ao6V1VvAceAA8MFVfV0Vf16sHgS2D7ZNiVNuy5htQ04P7S8MFi3nK8AP7yUpiRp1OYONVliXS1ZmHyeflh9bpnth4BDADt37uzYoiR127NaAHYMLW8HLowWJbkeeBQ4UFW/WuqBqmq2qnpV1ZuZmRmnX0lTqktYnQJ2J9mVZAtwB3B8uCDJTuBx4MtV9fLk25Q07VY8DKyqxST3Ak8Cm4CjVXU2yeHB9iPA14GPAQ8nAVisqt7atS1p2qRqydNPa67X69Xc3Ny6PLek9ZHk9Lg7Ml7BLqkJhpWkJhhWkppgWElqgmElqQmGlaQmGFaSmmBYSWqCYSWpCYaVpCYYVpKaYFhJaoJhJakJhpWkJhhWkppgWElqgmElqQmGlaQmGFaSmmBYSWqCYSWpCYaVpCYYVpKa0CmskuxN8lKS+ST3L7E9Sb492H4myY2Tb1XSNFsxrJJsAh4C9gF7gINJ9oyU7QN2D74OAY9MuE9JU67LntXNwHxVnauqt4BjwIGRmgPAd6rvJHB1kk9OuFdJU6xLWG0Dzg8tLwzWrbZGksa2uUNNllhXY9SQ5BD9w0SA/0nyfIfnb8FW4PX1bmJCrpRZrpQ54Mqa5dPj3rFLWC0AO4aWtwMXxqihqmaBWYAkc1XVW1W3G5SzbDxXyhxw5c0y7n27HAaeAnYn2ZVkC3AHcHyk5jhw1+C3grcAv6mqX47blCSNWnHPqqoWk9wLPAlsAo5W1dkkhwfbjwAngP3APPBb4O61a1nSNOpyGEhVnaAfSMPrjgzdLuCeVT737CrrNzJn2XiulDnAWQBIP2ckaWPz4zaSmrDmYXUlfVSnwyxfGsxwJsnTSW5Yjz5XstIcQ3WfSfJ2ktsvZ3+r0WWWJLcmeSbJ2SQ/vtw9dtXh++ujSX6Q5NnBLBvy3HCSo0leW+7SpLHf81W1Zl/0T8j/F/BHwBbgWWDPSM1+4If0r9W6BfjpWva0xrN8FrhmcHvfRpylyxxDdf9B/1zl7evd9yW8JlcDLwA7B8sfX+++L2GWfwC+Obg9A7wBbFnv3peY5S+BG4Hnl9k+1nt+rfesrqSP6qw4S1U9XVW/HiyepH+92UbT5TUB+CrwPeC1y9ncKnWZ5U7g8ap6FaCqNuo8XWYp4CNJAnyYflgtXt42V1ZVT9HvbTljvefXOqyupI/qrLbPr9D/6bHRrDhHkm3AF4EjbGxdXpNPAdck+VGS00nuumzdrU6XWR4ErqN/wfVzwNeq6p3L095EjfWe73TpwiWY2Ed1NoDOfSb5PP2w+tyadjSeLnN8C7ivqt7u/xDfsLrMshm4CbgN+CDwkyQnq+rltW5ulbrM8gXgGeCvgT8G/i3Jf1bVf69xb5M21nt+rcNqYh/V2QA69ZnkeuBRYF9V/eoy9bYaXeboAccGQbUV2J9ksaq+f1k67K7r99frVfUm8GaSp4AbgI0WVl1muRv4p+qf+JlP8gpwLfCzy9PixIz3nl/jE22bgXPALt49afgnIzV/x3tPtv1svU8QXsIsO+lfxf/Z9e73UuYYqX+MjXuCvctrch3w74PaDwHPA3+63r2POcsjwD8Obn8C+AWwdb17X2aeP2T5E+xjvefXdM+qrqCP6nSc5evAx4CHB3sli7XBPoDacY4mdJmlql5M8gRwBngHeLSqNtxf++j4ujwAPJbkOfpv9PuqasP9NYYk3wVuBbYmWQC+AXwALu097xXskprgFeySmmBYSWqCYSWpCYaVpCYYVpKaYFhJaoJhJakJhpWkJvwvA6w/ShZnbhIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for idx, blm in enumerate(blm_folder):\n",
    "    axn = fig.add_subplot(321+idx)\n",
    "    solution = pd.read_csv(f\"{MARXAN_FOLDER}/{blm}/output_best.csv\")\n",
    "    solution_grid = pu_area.merge(solution,left_on='PUID',right_on = 'PUID',how='inner')\n",
    "    solution_grid.plot(ax=axn,column='SOLUTION', legend=True)\n",
    "    axn.set_title(f'{blm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate optimum BLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each BLM scenario infer best solution from lowest score  \n",
    "- For each BLM scenario calculate Boundary Length of the best solution\n",
    "- Create df with values for best solution of each BLM scenario \n",
    "- Find the point of max curvature to establish as optimum BLM with [kneed](https://www.kaggle.com/kevinarvai/knee-elbow-point-detection) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kneed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-958af7ef6680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkneed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBLM_calibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARXAN_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPlot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mblm_df\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'folder'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'blm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kneed'"
     ]
    }
   ],
   "source": [
    "import kneed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def BLM_calibration(MARXAN_FOLDER, Plot=True):\n",
    "    blm_df =pd.DataFrame(columns=['folder','blm'])\n",
    "    blm_df['folder']= blm_folder\n",
    "    blm_df['blm']= blm_values\n",
    "\n",
    "    for blm in blm_folder:\n",
    "        summary = pd.read_csv(f\"{MARXAN_FOLDER}/{blm}/output_sum.csv\")\n",
    "        best =summary.loc[summary.loc[:]['Score'].idxmin(),'Run_Number']\n",
    "        cost=summary.loc[best-1,'Cost']\n",
    "        blm_df.loc[blm_df['folder']==blm,'cost']=cost\n",
    "    \n",
    "        solution = pd.read_csv(f\"{MARXAN_FOLDER}/{blm}/output_best.csv\")\n",
    "        solution_grid = pu_area.merge(solution,left_on='PUID',right_on = 'PUID',how='inner')\n",
    "        blm_df.loc[blm_df['folder']==blm,'boundary_length']=solution_grid.dissolve(by='SOLUTION')['geometry'].length[1] ## perimeter in m\n",
    "    \n",
    "    y = blm_df['boundary_length']\n",
    "    x = blm_df['cost']\n",
    "    kn = kneed.KneeLocator(x, y, curve='convex', direction='decreasing')\n",
    "    best_blm = blm_df.loc[blm_df['cost']==kn.knee,'blm'].values[0]\n",
    "    #blm_df.loc[blm_df['cost']==kn.knee,'folder'].values[0] #in which folder\n",
    "\n",
    "    print(f'The optimun BLM is {best_blm}')\n",
    "    \n",
    "    \n",
    "    if Plot==True:\n",
    "        plt.xlabel('cost')\n",
    "        plt.ylabel('boundary length')\n",
    "        plt.plot(x, y, 'bx-')\n",
    "        plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "        \n",
    "    return best_blm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BLM_calibration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-779d60d57273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBLM_calibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARXAN_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BLM_calibration' is not defined"
     ]
    }
   ],
   "source": [
    "BLM_calibration(MARXAN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to run Marxan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from shutil import rmtree\n",
    "\n",
    "from datetime import datetime\n",
    "import abc\n",
    "import collections\n",
    "from typing import Generic, TypeVar, Generic, Optional, List, Type\n",
    "from pydantic import BaseModel, ValidationError, validator, Field\n",
    "from pydantic.generics import GenericModel\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import qgrid\n",
    "from IPython.display import display, JSON\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import json\n",
    "\n",
    "from uuid import UUID, uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatFile(object):\n",
    "    \"\"\"\n",
    "    Read and write dat files.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str = None):\n",
    "        # Ensure the file has the right extension\n",
    "        if file_path and not file_path.endswith('.dat'):\n",
    "            raise NameError(\"File must be a '.dat' extension\")\n",
    "        \n",
    "        self.__path = file_path\n",
    "        self.data = None\n",
    "        \n",
    "    def __test_path(self, path):\n",
    "        if not self.__path and path:\n",
    "            self.__path = path\n",
    "        elif not self.__path and not path:\n",
    "            raise NameError('No path to file provided')\n",
    "    \n",
    "    def update(self, data, incremental: bool = False ):\n",
    "        \"\"\"Updates the data.\n",
    "    \n",
    "        Args:\n",
    "            data (any): The data the object should store.\n",
    "        Returns:\n",
    "            The contents of the file as a unicode string.\n",
    "        \"\"\"\n",
    "        __conversion: dict = {\n",
    "                                int: int,\n",
    "                                str: str,\n",
    "                                dict: dict,\n",
    "                                list: list\n",
    "                            }\n",
    "        if not incremental or not self.data:\n",
    "            self.data = data\n",
    "        elif type(self.data) == dict:\n",
    "            self.data.append(data)\n",
    "        else:\n",
    "            self.data = self.data + __conversion[type(self.data)](data)\n",
    "        \n",
    "    \n",
    "    def read(self, file_path: str = None):\n",
    "        \"\"\"Gets a files contents as a unicode string.\n",
    "    \n",
    "        Args:\n",
    "            filename (string): The full path to the file that will be read.\n",
    "        Returns:\n",
    "            The contents of the file as a unicode string.\n",
    "        \"\"\"\n",
    "        self.__test_path(file_path)\n",
    "        try:\n",
    "            with io.open(self.__path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "                self.data = f.readlines()\n",
    "        except (UnicodeDecodeError) as e:\n",
    "            with io.open(self.__path, mode=\"r\", encoding=\"ISO-8859-1\") as f:\n",
    "                self.data = f.read()\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def write(self, file_path: str = None):\n",
    "        \"\"\"Writes a files contents as a unicode string\n",
    "    \n",
    "        Args:\n",
    "            filename (string): The full path to the file that will be written.  \n",
    "            s (string): The unicode string to write.  \n",
    "            mode (string): Optional. The file write mode. Default value is w.  \n",
    "        Returns:\n",
    "            None  \n",
    "        \"\"\"\n",
    "        self.__test_path(file_path)\n",
    "        try:\n",
    "            with io.open(self.__path, mode, encoding=\"utf-8\") as f: \n",
    "                f.write(self.data)\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case we need to format the float numbers in a very specific way\n",
    "class MyNumber:\n",
    "    \"\"\"\n",
    "    Number formater wraper to allow a specific format string for a float number.\n",
    "    \"\"\"\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __format__(self,format_spec):\n",
    "        ss = ('{0:'+format_spec+'}').format(self.val)\n",
    "        if ( 'E' in ss):\n",
    "            mantissa, exp = ss.split('E')            \n",
    "            return mantissa + 'E'+ exp[0] + '00' + exp[1:]\n",
    "        return ss\n",
    "\n",
    "def num(s):\n",
    "    \"\"\"\n",
    "    Coerce Number transformation into float.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(s)\n",
    "        except ValueError:\n",
    "            return s\n",
    "\n",
    "def getSizeOfNestedList(listOfElem):\n",
    "    ''' Get number of elements in a nested list'''\n",
    "    count = 0\n",
    "    # Iterate over the list\n",
    "    for elem in listOfElem:\n",
    "        # Check if type of element is list\n",
    "        if type(elem) == list:  \n",
    "            # Again call this function to get the size of this element\n",
    "            count += getSizeOfNestedList(elem)\n",
    "        else:\n",
    "            count += 1    \n",
    "    return count\n",
    "\n",
    "def _readTabularFile(filename: str)-> dict:\n",
    "    \"\"\"\n",
    "    Gets a input.dat file and outputs a dict of parameters.\n",
    "\n",
    "    Args:\n",
    "        filename (string): The full path to the file that will be read.\n",
    "    Returns:\n",
    "        The contents of the file as a dict.\n",
    "    \"\"\"\n",
    "    inputData = DatFile(filename)\n",
    "    inputData.read()\n",
    "    outputData = []\n",
    "    \n",
    "    dialect= csv.Sniffer().sniff(inputData.data[0],[',', '\\t',' '])\n",
    "    for line in inputData.data: \n",
    "        if dialect.delimiter ==',':\n",
    "            pair= re.compile('\\s').split(line.replace('\"','').replace(' ', '_').strip('\"\"').strip('\\r').strip('\\n').replace(',', '\\t'))\n",
    "        else:\n",
    "            pair= re.compile('\\s').split(line.replace('\"','').strip('\"\"').strip('\\r').strip('\\n'))\n",
    "        outputData.append(pair)\n",
    "    return outputData\n",
    "\n",
    "\n",
    "def CreateListModelFromFile(filename: str, model: Type['Model'])-> List['Model']:\n",
    "    \"\"\"\n",
    "    Gets a input.dat file and outputs a list of selected model.\n",
    "\n",
    "    Args:\n",
    "        filename (string): The full path to the file that will be read.\n",
    "        model (Type['Model']): Data model used to read and validate the data\n",
    "    Returns:\n",
    "        The contents of the file as a Data model list.\n",
    "    \"\"\"\n",
    "    inputData = _readTabularFile(filename)\n",
    "    return [model.parse_obj(dict(zip(inputData[0], x))) for x in inputData[1:]] \n",
    "\n",
    "def CreateFileFromDF(filename: str, df: Type['Dataframe'], model: Type['Model'])-> List[list]:\n",
    "    \"\"\"\n",
    "    Gets a dataframe and outs a dat.file.\n",
    "\n",
    "    Args:\n",
    "        filename (string): The full path to the file that will be read.\n",
    "        df (Type['Dataframe']): Dataframe to save\n",
    "        model (Type['Model']): Data model used to read and validate the data\n",
    "    Returns:\n",
    "        The contents of the file as a Data model list.\n",
    "    \"\"\"\n",
    "    if model == inputDatFile:\n",
    "        data = df.transpose().to_dict('records')\n",
    "        validatedData = [inputDatFile(**x) for x in data]\n",
    "        keys = data[0].keys()\n",
    "        csv.register_dialect('dat', delimiter=' ')\n",
    "        with open(filename, 'w', encoding='utf8', newline='')  as output_file:\n",
    "            dict_writer = csv.writer(output_file, dialect='dat')\n",
    "            for row in data[0].items():\n",
    "                dict_writer.writerow(row)\n",
    "    else:\n",
    "        data = df.to_dict('records')\n",
    "        validatedData = [model(**x) for x in data]\n",
    "#         keys = validatedData[0].__dict__.keys()\n",
    "#         csv.register_dialect('dat', delimiter='\\t')\n",
    "#         with open(filename, 'w', encoding='utf8', newline='')  as output_file:\n",
    "#             dict_writer = csv.DictWriter(output_file, keys, dialect='dat')\n",
    "#             dict_writer.writeheader()\n",
    "#         dict_writer.writerows(toCSV)\n",
    "#             dict_writer.writerows(data)\n",
    "    \n",
    "        keys={k: v for k, v in data[0].items() if v is not None}.keys()\n",
    "        dataNotNone = list(({key : val for key, val in sub.items() if val!= None} for sub in data)) \n",
    "\n",
    "        csv.register_dialect('dat', delimiter=' ')\n",
    "        with open(filename, 'w', encoding='utf8', newline='')  as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys, dialect='dat')\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(dataNotNone)\n",
    "    \n",
    "    \n",
    "    return validatedData\n",
    "\n",
    "def save_button(filename: str, model: Type['Model'], data: Type['QgridWidget'])-> None:\n",
    "    \"\"\"\n",
    "    creates a widget button and attach a on click event.\n",
    "    \n",
    "    Args:\n",
    "        filename (string): The full path to the file that will be read.\n",
    "        data (Type['QgridWidget']): Qgrid widget\n",
    "        model (Type['Model']): Data model used to read and validate the data\n",
    "    \"\"\"\n",
    "    button = widgets.Button(description=\"Save\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    display(button, output)\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            CreateFileFromDF(filename, data.get_changed_df(), model)\n",
    "\n",
    "    button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputDatFile(BaseModel):\n",
    "    \"\"\"\n",
    "    This is the description of the input data clase base on marxan input file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # General Parameters\n",
    "    VERSION: str = Field('0.1', title='Version', \n",
    "                         description='Type of input file')\n",
    "    BLM:  Optional[float] = Field(0., title='Boundary Length Modifier', \n",
    "                           description='Boundary Length Modifier')\n",
    "    PROP: float = Field(0., title='Starting Proportion', \n",
    "                            description='Proportion of planning units in initial reserve system')\n",
    "    RANDSEED: Optional[int] = Field(-1, title='Random Seed', \n",
    "                              description='Random seed number')\n",
    "    NUMREPS: int = Field(1, title='Repeat Runs', \n",
    "                             description='The number of repeat runs you wish to do')\n",
    "    BESTSCORE: Optional[int] = Field(0, title='Best Score Speedup', \n",
    "                               description='This variable tells Marxan not to keep track of the best score \\\n",
    "                                until it reaches a specified minimum level.')\n",
    "    \n",
    "    # Annealing Parameters\n",
    "    NUMITNS: int = Field(0, title='Number of Iterations', \n",
    "                         description='Number of iterations for annealing')\n",
    "    STARTTEMP: int = Field(1, title='Initial Temperature', \n",
    "                           description='Starting temperature for annealing')\n",
    "    COOLFAC: int = Field(0, title='Cooling Factor', \n",
    "                         description='Cooling factor for annealing')\n",
    "    NUMTEMP: int = Field(1, title='Temperature Decreases', \n",
    "                         description='Number of temperature decreases for annealing')\n",
    "    \n",
    "    # Cost Threshold\n",
    "    COSTTHRESH: Optional[float] = Field(0, title='Threshold', \n",
    "                              description='Cost threshold')\n",
    "    THRESHPEN1: Optional[float] = Field(0, title='Penalty Factor A', \n",
    "                              description='Size of cost threshold penalty')\n",
    "    THRESHPEN2: Optional[float] = Field(0, title='Penalty Factor B', \n",
    "                              description='Shape of cost threshold penalty')\n",
    "    \n",
    "    # Input Files\n",
    "    INPUTDIR: str = Field('input', title='Input Folder', \n",
    "                          description='User Defined Name of the folder containing input data files')\n",
    "    SPECNAME: str = Field('spec.dat', title='Species File Name', \n",
    "                          description='Name of Conservation Feature File')\n",
    "    PUNAME: str = Field('pu.dat', title='Planning Unit File Name', \n",
    "                        description='Name of Planning Unit File')\n",
    "    PUVSPRNAME: str = Field('puvspr2.dat', title='Planning Unit versus Species', \n",
    "                            description='Name of Planning Unit versus Conservation Feature File')\n",
    "    BOUNDNAME: str = Field('bound.dat', title='Boundary Length', \n",
    "                           description='Name of Boundary Length File')\n",
    "    BLOCKDEFNAME: Optional[str] = Field('blockdef.dat', title='Block Definitions', \n",
    "                              description='Name of Block Definition File')\n",
    "    \n",
    "    # Output Files\n",
    "    VERBOSITY: int =  Field(1, title='Screen Output', \n",
    "                            description='Amount of output displayed on the program screen')\n",
    "    MISSLEVEL: Optional[float] =  Field(1, title='Species missing proportion', \n",
    "                              description='Amount or target below which it is counted as ‘missing’')\n",
    "    OUTPUTDIR: str = Field('output', title='', \n",
    "                           description='User Defined Name of the folder in which to save output files')\n",
    "    SCENNAME: str = Field('Default_name', title='Scenario name', \n",
    "                          description='Scenario name for the saved output files')\n",
    "    SAVERUN: Optional[int] = Field(3, title='Save each run', \n",
    "                         description='Save each run? (0 = no)')\n",
    "    SAVEBEST: Optional[int] =  Field(3, title='Save the best run', \n",
    "                           description='Save the best run? (0 = no)')\n",
    "    SAVESUMMARY: Optional[int] =  Field(3, title='Save summary', \n",
    "                          description='Save summary information? (0 = no)')\n",
    "    SAVESCEN: Optional[int] =  Field(3, title='Save scenario', \n",
    "                           description='Save scenario information? (0 = no)')\n",
    "    SAVETARGMET: Optional[int] =  Field(3, title='Save targets met', \n",
    "                              description='Save targets met information? (0 = no)')\n",
    "    SAVESUMSOLN: Optional[int] =  Field(3, title='', \n",
    "                              description='Save summed solution information? (0 = no)')\n",
    "    SAVELOG: Optional[int] =  Field(1, title='Save summed solution', \n",
    "                          description='Save log files? (0 = no)')\n",
    "    SAVESNAPSTEPS: Optional[int] =  Field(0, title='Save snapshots', \n",
    "                                description='Save snapshots each n steps (0 = no)')\n",
    "    SAVESNAPCHANGES: Optional[int] =  Field(0, title='Save snapshots changes', \n",
    "                                  description='Save snapshots after every n changes (0 = no)')\n",
    "    SAVESNAPFREQUENCY: Optional[int] =  Field(0, title='Frequency of snapshots', \n",
    "                                    description='Frequency of snapshots if they are being used')\n",
    "    SAVESOLUTIONSMATRIX: Optional[int] =  Field(3, title='Frequency of snapshots', \n",
    "                                    description='Frequency of snapshots if they are being used')\n",
    "    \n",
    "    # Program control.\n",
    "    RUNMODE: int = Field(1, title='Run Options', \n",
    "                         description='User Defined The method Marxan uses to find solutions')\n",
    "    \n",
    "    ITIMPTYPE: int =  Field(1, title='Iterative Improvement', \n",
    "                            description='Iterative improvement type')\n",
    "    HEURTYPE: int =  Field(1, title='Heuristic', \n",
    "                           description='Heuristic type')\n",
    "    CLUMPTYPE: Optional[int] =  Field(0, title='Clumping Rule', \n",
    "                            description='Clumping penalty type')\n",
    "    \n",
    "    # class Config:\n",
    "    @validator('SAVERUN')\n",
    "    def SAVERUN_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVEBEST')\n",
    "    def SAVEBEST_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVESUMMARY')\n",
    "    def SAVESUMMARY_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVESCEN')\n",
    "    def SAVESCEN_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVETARGMET')\n",
    "    def SAVETARGMET_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVESUMSOLN')\n",
    "    def SAVESUMSOLN_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVELOG')\n",
    "    def SAVELOG_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('SAVESOLUTIONSMATRIX')\n",
    "    def SAVESOLUTIONSMATRIX_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'No file generated',\n",
    "                      1: 'save a file as .dat',\n",
    "                      2: 'save a file as .txt',\n",
    "                      3: 'save a file as .csv'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('RUNMODE')\n",
    "    def RUNMODE_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'Apply Simulated Annealing followed by a Heuristic',\n",
    "                      1: 'Apply Simulated Annealing followed by Iterative Improvement',\n",
    "                      2: 'Apply Simulated Annealing followed by a Heuristic, followed by Iterative',\n",
    "                      3: 'Use only a Heuristic',\n",
    "                      4: 'Use only Iterative Improvement',\n",
    "                      5: 'Use a Heuristic followed by Iterative Improvement',\n",
    "                      6: 'Use only Simulated Annealing'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('ITIMPTYPE')\n",
    "    def ITIMPTYPE_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {0: 'Normal Iterative Improvement',\n",
    "                      1: 'Two Step Iterative Improvement',\n",
    "                      2: '‘Swap’ Iterative Improvement',\n",
    "                      3: 'Normal Improvement followed by Two Step Iterative Improvement'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('HEURTYPE')\n",
    "    def HEURTYPE_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {-1:'Ignored',\n",
    "                      0: 'Richness',\n",
    "                      1: 'Greedy',\n",
    "                      2: 'Max Rarity',\n",
    "                      3: 'Best Rarity',\n",
    "                      4: 'Average Rarity',\n",
    "                      5: 'Sum Rarity',\n",
    "                      6: 'Product Irreplaceability',\n",
    "                      7: 'Summation Irreplaceability'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('VERBOSITY')\n",
    "    def VERBOSITY_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {-1:'Ignored',\n",
    "                      0: 'Silent Running',\n",
    "                      1: 'Results Only',\n",
    "                      2: 'General Progress',\n",
    "                      3: 'Detailed Progress'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    @validator('CLUMPTYPE')\n",
    "    def CLUMPTYPE_is_valid(cls, method: int) -> int:\n",
    "        allowed_set = {-1:'Ignored',\n",
    "                       0: 'Partial clumps do not count',\n",
    "                       1: 'Partial clumps count half',\n",
    "                       3: 'Graduated penalty'}\n",
    "        \n",
    "        if method not in allowed_set.keys():\n",
    "            raise ValueError(f\"must be in {allowed_set}, got '{method}'\")\n",
    "        \n",
    "        return method\n",
    "    \n",
    "    def to_dat(self):\n",
    "        \"\"\"\n",
    "        Gets a input.dat file and outputs a dict of parameters.\n",
    "\n",
    "        Args:\n",
    "            \n",
    "        Returns:\n",
    "            The data model converted on a readeable dat string.\n",
    "        \"\"\"\n",
    "        s:str = ''\n",
    "        for key, value in self.dict().items():\n",
    "            #add the key\n",
    "            s = s + key + \" \" + str(value) + \"\\n\"\n",
    "        return s\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dat(cls: Type['Model'], dat: str)-> 'Model':\n",
    "        \"\"\"\n",
    "        Gets a input.dat file and outputs a dict of parameters.\n",
    "\n",
    "        Args:\n",
    "            filename (string): The full path to the file that will be read.\n",
    "        Returns:\n",
    "            The contents of the file as a dict.\n",
    "        \"\"\"\n",
    "        obj = {}\n",
    "        \n",
    "        for line in dat:\n",
    "            if re.search('[A-Z1-9_]{2,}', line, re.DOTALL):\n",
    "                pair = line.strip('\\r').strip('\\n').split(' ')\n",
    "                pair =list(filter(None, pair)) ## delete empty lists\n",
    "                if len(pair)>2: #remove lists that have more values\n",
    "                    continue\n",
    "                assert len(pair) == 2 # if the list has more or less attribute than 2 it means we have make a mistake spliting stuff\n",
    "                obj[pair[0]] = num(pair[1].strip(' '))\n",
    "                \n",
    "        return cls.parse_obj(obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
