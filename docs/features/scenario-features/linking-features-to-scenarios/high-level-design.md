# Linking features to scenarios - refactor - High-level design

The initial MarxanCloud data model links features (species or bioregional) to
scenarios via the `(geodb)scenario_features_data`, which references the
`(geodb)features_data` table via a foreign key, and the `(apidb)scenarios` table
via plain UUID (no constraint is enforced as the reference is across database
boundaries).

Although this setup is suitable for the preparation of Marxan input files, a
refactor is needed in order to enable to reliably link features to scenarios
when cloning scenarios or exporting/importing projects.

## Limitations of the current design

Platform-wide features and user-uploaded features are both listed in the
`(apidb)features` table.

For platform-wide features, `(apidb)features.project_id` is null (and
`(apidb)features.is_custom` is `false`, as a generated column based on the
former).

For user-uploaded features, conversely, `(apidb)features.project_id` references
`(apidb)projects.id` and `(apidb)features.is_custom` is `true`.

Features are linked to scenarios in the `geodb` database:

- for each `(apidb)features` record, there will be one or more
  `(geodb)features_data` records, where `(geodb)features_data.feature_id`
  "references" `(apidb)features.id`.

- when a feature is used in a scenario, its relevant `(geodb)features_data`
  records are linked to the scenario via `(geodb)scenario_features_data` records

Feature metadata is therefore exported at project level, whereas the links
between specific features and each scenario of a project (alongside Marxan
parameters such as `prop` and `fpf` for each feature selected in a scenario) are
exported at scenario level.

Since scenario export pieces always run _after_ the relevant project export
pieces (and likewise, scenario import pieces always run after project import
pieces), the current design does not allow to retain through a cloning operation
the scenario/feature links that are stored in the source scenarios: all the
`id`s involved in the process are `UUIDv4`s automatically generated by
PostgreSQL at `INSERT` time, and it is therefore impossible to keep the
scenario/feature links through these `id`s only, as they are not stable by
definition across different projects/scenarios (for example, when a project is
duplicated, as the copy of all the `(geodb)features_data` and
`(geodb)scenario_features_data` records will have new, automatically generated
`id`s).

## Aims and assumptions

The refactor described in this document is limited to the following aims:

- provide stable, numeric `id`s that can be reliably referenced within each of
  an arbitrary number of copies of a project and its scenarios, whether in the
  same MarxanCloud instance or across different instances.

- allow to reliably link features to scenarios across cloning operations, both
  for platform-wide and for user-uploaded features.

Additionally, the refactor relies on the following assumptions:

- projects and their scenarios are locked for edits during the entire duration
  of an export (i.e. since the export process has started and until it
  successfully completes or terminates with errors), and likewise locked for
  edits during the entire duration of an import (i.e. cannot be changed in any
  way until the project being cloned and all its scenarios have been fully
  imported, or until a scenario has been fully imported if the user is cloning
  an individual scenario).

- reliable linking of features to scenarios may be guaranteed only across the
  timespan of a single clone operation (and distinctly for the export and for
  the import sides of it): successive clones of the same project may allow for
  different feature sets to be reliably linked, if features are
  added/deleted/changed in the source project.

- platform-wide features are considered immutable for the purposes of the
  present specification: for the time being, there are no provisions through the
  MarxanCloud API for any traits of features to be changed after their initial
  import via ETL pipelines; as a corollary, if the stable identifier used for a
  platform-wide feature defined in `(apidb)features` (see below:
  `project_id/feature_class_name`) cannot be resolved to a feature in the target
  MarxanCloud instance, if different from the source one, this should be
  considered an error (as all platform-wide features referenced in an exported
  project are required to be available in the target platform, if different from
  the source one)

- one or more import steps of a cloning or import process could be ongoing at
  the same time for a given project and its scenarios

## Implementation - summary

This section presents a summary of the process, with various extents of
pseudocode/pseudosql to aid with the identification of the tables/columns
involved.

For final implementation details, always refer to the code: this document's
scope is only to guide the development of the refactor, not to be an overview of
the actual implementation nor to be kept up to date as the implementation may
evolve after the initial refactor.

### Data schema changes

Features should be guaranteed to be unique either across the space of
platform-wide features, or across each project.

This can be enforced via partial unique indexes in `apidb`, e.g.

```
create unique index unique_platform_features on features (feature_class_name) where project_id is null;
create unique index unique_project_features on features (feature_class_name, project_id) where project_id is not null;
```

Likewise, in order to be able to reliably identify `(geodb)features_data`, we
need a unique hash within each project (or platform-wide `(geodb)features_data`
rows):

```
alter table features_data add column hash text not null generated always as (md5(the_geom::bytea::text || properties::text)) stored;
create unique index unique_feature_data_per_project on features_data(hash, feature_id);
```

Other than these changes, this proposal seeks to retain the current data
architecture in order to minimize impact on existing code/queries and related
risks.

### User-uploaded features - export

- At _project export time_, for each `(apidb)features` for the given project, as
  `current_feature`:

  - Export `(apidb)features` data, adding to the projection a generated column
    on the fly: `source_feature = '[project|platform]/<feature_class_name>'`
    (@todo revie: likely not needed)

  - Export `(geodb)features_data` where `features_data.feature_id =
    current_feature.id`, adding to the projection a generated column on the
    fly: `source_feature = '[project|platform]/<feature_class_name>'`; this will
    be used as a back-reference to link `features_data` rows to their matching
    `(apidb)features` rows at import time.

- At _scenario export time_, for each distinct
  `(geodb)features_data.feature_id`:

  - Check if `(geodb)features_data.feature_id` for one of the related
    `(geodb)features_data` rows maps to a user-uploaded `(apidb)features` row;
    if so, with this as `current_feature`:

  - Check if any of the feature's data rows are linked to the scenario:

```
select count(*) from scenario_features_data where feature_class_id in (select id from features_data where feature_id = current_feature.id);
```

If not, this feature is not in use in the scenario being exported, so we can
skip to the next `(geodb)features_data.feature_id`.

If the feature is in use:

  - Export `(geodb)scenario_features_data where feature_class_id in (select id
    from features_data where feature_id = current_feature.id)`, adding to the
    projection a generated column on the fly: `source_feature_data =
    '<feature_class_name>/<hash>'` of the related `(geodb)features_data` row;
    this will be used as a back-reference to link `scenario_features_data` rows
    to their matching `features_data` rows at import time.

### Platform-wide features - export

For these features, we rely on the core assumption for platform-wide features
outlined in the _Aims and assumptions_ section above.

As all the project-level metadata and spatial data for these features must be
already present in the target MarxanCloud instance (this will be true by
definition for cloning operations, as these are performed within the same
instance), only `(geodb)scenario_features_data` rows will be exported.

The export process, at scenario export time, is mostly identical to that of
`(geodb)scenario_features_data` rows for user-uploaded `(apidb)features`, except
that we iterate over platform-wide `(apidb)features` rows and we reference
`(geodb)features_data` rows that map to platform-wide data.

### User-uploaded features - import

- Given `new_project_id` as the id of the project created through the current
  import process;

- At _project import time_, for each `(apidb)features` in the import piece, as
  `current_feature`:

  - Import `(apidb)features`

  - For each `(apidb)features` row as `current_feature`, import
    `(geodb)features_data` for it: these are the exported `features_data` rows
    for the project `where replace(source_feature, 'project/', '') =
    current_feature.feature_class_name`

    - For each `(geodb)features_data` row being inserted, set `feature_id` to
      `select id from (apidb)features where project_id = <new_project_id> and
      feature_class_name = replace(source_feature, 'project/', '')`

- At _scenario import time_, for each distinct `(apidb)features` in the import
  piece, as `current_feature`:

  - Import `(geodb)scenario_features_data` for it: these are the exported rows
    `where regexp_replace(source_feature_data, '\/[0-9a-fA-F]+$', '') =
    <current_feature.feature_class_name>`.

    - For each `(geodb)scenario_features_data` row being inserted, set
      `feature_class_id` to `select id from features_data where feature_id =
      current_feature.id and hash = replace(source_feature_data,
      '<feature_class_name>/', '')`

### Platform-wide features - import

As for the export of platform-wide features, this step is largely identical to
the analogous step (at _scenario import time_) for user-uploaded features.

There is no import step for `(apidb)features` rows and `(geodb)features_data`
rows as these must be already present in the target instance.

Given that the exported data for `(geodb)scenario_features_data` includes a
generated column that references `(apidb)features.feature_class_name`, we can
iterate over the relevant `(apidb)features where project_id is null`, and for
each of the `(geodb)scenario_features_data` rows in the exported piece, import
the row, setting its `feature_class_id` column to `select id from
(geodb)feature_data where feature_id = current_feature.id and hash =
replace(source_feature_data, '<feature_class_name>/', '')`.

## Implementation (full rationale)

This section outlines some implementation details originally meant as guidance
to the implementation of the refactor; its main reference value should be the
rationale behind the suggestions outlined, irrespective of the implementation
choices that ended up being taken during development.

### Reliably identifying features referenced from feature geometries

1. Features should be guaranteed to be unique either across the space of
   platform-wide features, or across each project.

This can be enforced via partial unique indexes in `apidb`.

2. *At export time*, a unique combined `[project|platform]/feature_class_name`
   stable identifier should be added to the exported `(apidb)features` rows via
   the export piece for project features; this identifier is then used to
   reference `(apidb)features` in the export piece for scenario features.

For example, such `id`s will look like `platform/equus_quagga` or
`project/equus_quagga`.

These identifiers are expected to be stable:

- for platform-wide features, their `feature_class_name` is not meant to ever
  change
- for user-uploaded features, their `feature_class_name` is guaranteed to be
  stable throughout an export or import step of a clone operation, thanks to
  locks in place to prevent editing projects that are being exported or imported

3. *At project export time*, the generated export data for
   `(geodb)features_data` should then _also_ reference the combined
   `[project|platform]/feature_class_name` identifier added at export time to
   `(apidb)features`.

This should only be done at export time; permanently persisting these
`[project|platform]/feature_class_name` identifiers in `(geodb)features_data`
would essentially duplicate the current role of the
`(geodb)features_data.feature_id` column, and would need additional logic to
keep this column in sync with the `(apidb)features` table, for example in case
`(apidb)features.feature_class_name` is updated.

### Reliably identifying feature geometries from scenarios

The general strategy used here is the introduction of a stable, univocal and
monotonic numeric identifier for each `(geodb)features_data` record linked to a
given `(apidb)feature`, so that rows could be identified and linked between
project-level and scenario-level exported data.

1. Platform `(geodb)features_data` records should be extended to include an
   identity column, through which these records can be ordered and referenced in
   a stable way, for example:

```
alter table features_data add column fdid bigint not null generated always as identity;
create index concurrently features_data_fdid on features_data (feature_id, fdid asc);
```

The index will help in non-trivial cases with selecting `(geodb)features_data`
records sorted by `fdid`.

Incidentally, it may be a good idea to update the
`precompute_feature_property_list()` function (added in migration
`geoprocessing/RefactorExtractDistinctPropsFromFeatures1625042393000`)
beforehand in order to avoid pointless duplicated fan-out of JSON properties
into the `(geodb)feature_properties_kv` table.

2. *At project export time*, a unique combined
   `feature_class_data.feature_id/hash` stable identifier should be added to the
   exported `(geodb)features_data` rows via the export piece for project
   features; this identifier is then used to reference `(geodb)features_data` in
   the export piece for scenario features; this identifier should build 

3. *At scenario export time*, the generated export data for
   `(geodb)scenario_features_data` should include the identifier above in order
   for the import step to be able to reliably link each
   `(geodb)scenario_features_data` row to the corresponding
   `(geodb)features_data` row.
